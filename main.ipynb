{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Pose.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import Pose as p\n",
    "import cv2\n",
    "import time \n",
    "import numpy as np\n",
    "from fastdtw import fastdtw  #library to calculate the distance using dynamic time wraping \n",
    "from dtaidistance import dtw_ndim\n",
    "from dtaidistance import dtw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play():\n",
    "    \"\"\"\n",
    "    Function to read best dance video file and further use mediapipe to get 33 key points coordinates for every frame\n",
    "    and append the same to the list. Final list is returned.\n",
    "\n",
    "    Arguments: No arguments\n",
    "\n",
    "    returns : List of 33 key points for each frame. Shape (FRAMES,33 ,2)\n",
    "    \n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture('./video/5.mp4')\n",
    "    ############## change color in pose for video 2##################3\n",
    "    pTime=0\n",
    "    detector = p.poseDetect()\n",
    "    landmarks=[]\n",
    "    a=0\n",
    "    m=[]\n",
    "    while True:\n",
    "        ret,frame= cap.read()\n",
    "        a= a+1\n",
    "        success,img =cap.read()\n",
    "        if success==True:\n",
    "            img = detector.findPose(img)\n",
    "            res = detector.getPosition(img,draw=True)\n",
    "            m.append(res)\n",
    "            cTime =time.time()\n",
    "            fps=1/(cTime-pTime)\n",
    "            pTime= cTime\n",
    "            cv2.putText(img,str(int(fps)),(70,50),cv2.FONT_HERSHEY_PLAIN,3, (255,0,0),3)\n",
    "            cv2.imshow(\"image\",img)\n",
    "        else:\n",
    "            return m\n",
    "        cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture():\n",
    "    \"\"\"\n",
    "    function to capture live video of 5 seconds which is processed using mediapipe to find coordinates for 33 key points \n",
    "    in every frame. List is returned with shape (FRAMES,33,2) \n",
    "\n",
    "    Arguments : None\n",
    "\n",
    "    Returns : List of 33 key points for all frames. Shape (FRAMES, 33, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    detector = p.poseDetect()\n",
    "    cam_arr= []\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ret= cap.set(3,640)\n",
    "    ret= cap.set(4,352)\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        ret,frame = cap.read()\n",
    "        frame = detector.findPose(frame)\n",
    "        res = detector.getPosition(frame,draw=True)\n",
    "        cam_arr.append(res)\n",
    "        cv2.imshow(\"image\",frame)        \n",
    "        end = time.time()\n",
    "        if(int(end)-int(start) ==5):\n",
    "            break\n",
    "        cv2.waitKey(1)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows\n",
    "    return cam_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(arr):\n",
    "    \"\"\" \n",
    "    arguments :-\n",
    "    arr : array containing all 33 key points for all frames\n",
    "\n",
    "    returns : list containing 33 2D arrays of shape (FRAMES,2) \n",
    "    \n",
    "    \"\"\"\n",
    "    t= np.array(arr)\n",
    "    data= []\n",
    "    for i in range(t.shape[1]):\n",
    "        r=[]\n",
    "        for j in range(t.shape[0]):\n",
    "            r.append(t[j][i])\n",
    "        data.append(np.array(r))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_score_cal(res,acc):\n",
    "    \"\"\"\n",
    "    function used to calulate final score using 2 array which contains 33 key points of tutor and student video\n",
    "\n",
    "    arguments :\n",
    "    res : list of length 33, which 2d arrays of shape (FRAMES,2)\n",
    "    acc : list of length 33, which 2d arrays of shape (FRAMES,2)\n",
    "\n",
    "    returns : \n",
    "    final score : Float value of DTW distance\n",
    "    \"\"\"\n",
    "    score=[]\n",
    "    for i in range(33):\n",
    "        distance, path = fastdtw(res[i], acc[i])\n",
    "        score.append(distance)\n",
    "    range_min = min(score)\n",
    "    range_max = max(score)\n",
    "    range_ = range_max - range_min\n",
    "    mean =np.mean(score)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model to /home/ayush/anaconda3/envs/motion-env/lib/python3.9/site-packages/mediapipe/modules/pose_landmark/pose_landmark_lite.tflite\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_bool(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: bool) -> mediapipe.python._framework_bindings.packet.Packet\n\nInvoked with: 0.5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     result \u001b[39m=\u001b[39m play()\n\u001b[1;32m      3\u001b[0m     res \u001b[39m=\u001b[39m norm(result)\n\u001b[1;32m      4\u001b[0m     arr \u001b[39m=\u001b[39m capture()\n",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m, in \u001b[0;36mplay\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m############## change color in pose for video 2##################3\u001b[39;00m\n\u001b[1;32m     13\u001b[0m pTime\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m detector \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mposeDetect()\n\u001b[1;32m     15\u001b[0m landmarks\u001b[39m=\u001b[39m[]\n\u001b[1;32m     16\u001b[0m a\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m<string>:18\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, mode, upBody, smooth, detectioncon, trackcon)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/motion-env/lib/python3.9/site-packages/mediapipe/python/solutions/pose.py:146\u001b[0m, in \u001b[0;36mPose.__init__\u001b[0;34m(self, static_image_mode, model_complexity, smooth_landmarks, enable_segmentation, smooth_segmentation, min_detection_confidence, min_tracking_confidence)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initializes a MediaPipe Pose object.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m    https://solutions.mediapipe.dev/pose#min_tracking_confidence.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m _download_oss_pose_landmark_model(model_complexity)\n\u001b[0;32m--> 146\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    147\u001b[0m     binary_graph_path\u001b[39m=\u001b[39;49m_BINARYPB_FILE_PATH,\n\u001b[1;32m    148\u001b[0m     side_inputs\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    149\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mmodel_complexity\u001b[39;49m\u001b[39m'\u001b[39;49m: model_complexity,\n\u001b[1;32m    150\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39msmooth_landmarks\u001b[39;49m\u001b[39m'\u001b[39;49m: smooth_landmarks \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m static_image_mode,\n\u001b[1;32m    151\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39menable_segmentation\u001b[39;49m\u001b[39m'\u001b[39;49m: enable_segmentation,\n\u001b[1;32m    152\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39msmooth_segmentation\u001b[39;49m\u001b[39m'\u001b[39;49m:\n\u001b[1;32m    153\u001b[0m             smooth_segmentation \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m static_image_mode,\n\u001b[1;32m    154\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39muse_prev_landmarks\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mnot\u001b[39;49;00m static_image_mode,\n\u001b[1;32m    155\u001b[0m     },\n\u001b[1;32m    156\u001b[0m     calculator_params\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    157\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mposedetectioncpu__TensorsToDetectionsCalculator.min_score_thresh\u001b[39;49m\u001b[39m'\u001b[39;49m:\n\u001b[1;32m    158\u001b[0m             min_detection_confidence,\n\u001b[1;32m    159\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mposelandmarkbyroicpu__tensorstoposelandmarksandsegmentation__ThresholdingCalculator.threshold\u001b[39;49m\u001b[39m'\u001b[39;49m:\n\u001b[1;32m    160\u001b[0m             min_tracking_confidence,\n\u001b[1;32m    161\u001b[0m     },\n\u001b[1;32m    162\u001b[0m     outputs\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mpose_landmarks\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpose_world_landmarks\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msegmentation_mask\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/anaconda3/envs/motion-env/lib/python3.9/site-packages/mediapipe/python/solution_base.py:289\u001b[0m, in \u001b[0;36mSolutionBase.__init__\u001b[0;34m(self, binary_graph_path, graph_config, calculator_params, graph_options, side_inputs, outputs, stream_type_hints)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mfor\u001b[39;00m stream_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    287\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39mobserve_output_stream(stream_name, callback, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 289\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_side_packets \u001b[39m=\u001b[39m {\n\u001b[1;32m    290\u001b[0m     name: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_side_input_type_info[name], data)\n\u001b[1;32m    291\u001b[0m     \u001b[39mfor\u001b[39;00m name, data \u001b[39min\u001b[39;00m (side_inputs \u001b[39mor\u001b[39;00m {})\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    292\u001b[0m }\n\u001b[1;32m    293\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39mstart_run(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_side_packets)\n",
      "File \u001b[0;32m~/anaconda3/envs/motion-env/lib/python3.9/site-packages/mediapipe/python/solution_base.py:290\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mfor\u001b[39;00m stream_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    287\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39mobserve_output_stream(stream_name, callback, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    289\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_side_packets \u001b[39m=\u001b[39m {\n\u001b[0;32m--> 290\u001b[0m     name: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_packet(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_side_input_type_info[name], data)\n\u001b[1;32m    291\u001b[0m     \u001b[39mfor\u001b[39;00m name, data \u001b[39min\u001b[39;00m (side_inputs \u001b[39mor\u001b[39;00m {})\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    292\u001b[0m }\n\u001b[1;32m    293\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39mstart_run(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_side_packets)\n",
      "File \u001b[0;32m~/anaconda3/envs/motion-env/lib/python3.9/site-packages/mediapipe/python/solution_base.py:592\u001b[0m, in \u001b[0;36mSolutionBase._make_packet\u001b[0;34m(self, packet_data_type, data)\u001b[0m\n\u001b[1;32m    589\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(packet_creator, \u001b[39m'\u001b[39m\u001b[39mcreate_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m packet_data_type\u001b[39m.\u001b[39mvalue)(\n\u001b[1;32m    590\u001b[0m       data, image_format\u001b[39m=\u001b[39mimage_frame\u001b[39m.\u001b[39mImageFormat\u001b[39m.\u001b[39mSRGB)\n\u001b[1;32m    591\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(packet_creator, \u001b[39m'\u001b[39;49m\u001b[39mcreate_\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m packet_data_type\u001b[39m.\u001b[39;49mvalue)(data)\n",
      "\u001b[0;31mTypeError\u001b[0m: create_bool(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: bool) -> mediapipe.python._framework_bindings.packet.Packet\n\nInvoked with: 0.5"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    result = play()\n",
    "    res = norm(result)\n",
    "    arr = capture()\n",
    "    acc = norm(arr)\n",
    "    score=final_score_cal(res,acc)\n",
    "    print(\"Final Score is : \",score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
